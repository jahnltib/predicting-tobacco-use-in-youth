{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Predicting Factors of Tobacco Use in the Youth\n",
    "(Exploratory Data Analysis and Data Preproccessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Do this before running any code cell)\n",
    "1. While in VSCode, use command `cmd+shift+p` \n",
    "2. Select `Python: Create Environment` -> `Venv`. This will create a python venv to install all your python packages in. After creating it VSCode will automatically active it.\n",
    "3. Run command in **Install Packages** below to automatically install all required packages from the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n",
    "Install all the required packages directly from the requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to install required packages\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Imports\n",
    "Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/n7sr27qx5h18n91g81tmnsdh0000gn/T/ipykernel_13559/3635841.py:12: DtypeWarning: Columns (690,691,692,703,706,708,709,710,711,712,713,714,715,716,717,718,719,720,721,1172,1174,1185,1214) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/nyts2023.csv')\n",
      "/var/folders/4z/n7sr27qx5h18n91g81tmnsdh0000gn/T/ipykernel_13559/3635841.py:13: DtypeWarning: Columns (690,691,692,703,706,1185,1214,1247,1266,1285,1287,1298,1299,1303) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tobacco_user_df = pd.read_csv('../data/tobacco_users.csv')\n",
      "/var/folders/4z/n7sr27qx5h18n91g81tmnsdh0000gn/T/ipykernel_13559/3635841.py:14: DtypeWarning: Columns (690,691,692,703,704,1172,1173,1174,1185,1214) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nonuser_df = pd.read_csv('../data/nonusers.csv')\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from helpers.drop_list import dropped\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/nyts2023.csv')\n",
    "tobacco_user_df = pd.read_csv('../data/tobacco_users.csv')\n",
    "nonuser_df = pd.read_csv('../data/nonusers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update requirements.txt\n",
    "If you install any new packages, run this update the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Target Variable\n",
    "The target label for our model will be a binary classification of tobacco user or non-user. This label is based on Q100: \"During the past 30 days, on how many days did you use any tobacco product(s)?\". Respondents with a response value of 1 or greater will be labeled as tobacco users. Respondents either skipped Q100 or reported a value of 0 will be labeled as nonusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where Q100 is either 0, skipped, or missing\n",
    "# Keep only rows where Q100 has a numeric value of 1 or greater, indicating Tobacco Use\n",
    "\n",
    "tobacco_user_df = df[df['Q100'].apply(lambda x: str(x).isdigit() and int(x) >= 1)]\n",
    "\n",
    "# Count the number of respondents who use Tobacco\n",
    "num_respondents = len(tobacco_user_df)\n",
    "print(f\"Number of respondents who use Tobacco: {num_respondents}\")\n",
    "\n",
    "# Create a DataFrame for non-users by negating the condition for tobacco use\n",
    "nonuser_df = df[~df['Q100'].apply(lambda x: str(x).isdigit() and int(x) >= 1)]\n",
    "\n",
    "# Count the number of respondents who do not use Tobacco\n",
    "num_respondents_non = len(nonuser_df)\n",
    "print(f\"Number of respondents who do not use Tobacco: {num_respondents_non}\")\n",
    "\n",
    "# Export the filtered data to new CSV files\n",
    "tobacco_user_df.to_csv('tobacco_users.csv', index=False)\n",
    "nonuser_df.to_csv('nonusers.csv', index=False)\n",
    "print(\"Export Success.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison between tobacco users and non-users\n",
    "user_counts = [len(tobacco_user_df), len(nonuser_df)]\n",
    "labels = ['Tobacco Users', 'Non-Users']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, user_counts, color=['steelblue', 'lightcoral'])\n",
    "plt.title('Number of Respondents - Tobacco Users vs Non-Users')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mapping = {\n",
    "    1: '9 years old',\n",
    "    2: '10 years old',\n",
    "    3: '11 years old',\n",
    "    4: '12 years old',\n",
    "    5: '13 years old',\n",
    "    6: '14 years old',\n",
    "    7: '15 years old',\n",
    "    8: '16 years old',\n",
    "    9: '17 years old',\n",
    "    10: '18 years old',\n",
    "    11: '19 years old or older'\n",
    "}\n",
    "\n",
    "#showing actual ages instead of index\n",
    "df['age_labeled'] = df['QN1'].map(age_mapping)\n",
    "\n",
    "#plotting the proportion of respondents by age\n",
    "df['age_labeled'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(8, 8))\n",
    "plt.title('Proportion of Respondents by Age')\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "For this phase I didn't do much visualizes I just did a manual deep dive through the questions so far and took the notes below. I created two files `map.py` and `map_annotated.py` containing the original mapping of questions to columnID and an annotated version where I decided to note which columns should be merged, one-hot encoded, or just removed. \n",
    "\n",
    "**Notes:**\n",
    "- Many multiple choice questions are already split into separate columns and do not need to be one-hot encoded. But remove any Dummy Variable Trap questions.\n",
    "- Questions that are noted as Categorical need to be one-hot encoded. For example, for QN1, the categories are 0-13, 14-18, 19+, each question needs to be it's own column and have 0 or 1 if they belong to that group.\n",
    "- MERGE to NC means merge to new column, TC = target column\n",
    "  - A MERGE means that if they answered to any of these questions, their value to the new question would be 1. If they didn't answer to any of these questions, their value would be 0.\n",
    "- Questions like \"Why do you currently use e-cigarettes? (They are available in flavors, such as menthol, mint, candy, fruit, or chocolate)\" can be thought of as \n",
    "  \"Responded used e-cigarettes due to it's availability in flavors such as menthol, mint, candy, fruit, or chocolate\".\n",
    "- Skip Logic Questions don't really seem to be a problem as question answers are split into separate columns (as if they were one-hot encoded already) and dummy variable traps can be removed.\n",
    "  All other questions can seemingly be one-hot encoded (split into categories), merged (multiple of them) into new columns, or just removed because they are nto relevant to the analysis.\n",
    "  - The few exceptions of skip logic questions are ones that are removed anyways\n",
    "- Some questions like 48-51 on their own are weird to predict if they relate to tobacco use. For example (Q48) if you are curious about trying a cig\n",
    "  it wouldnt be significant to predict if you are a tobacco user. But if you combined it with smoking in the household, it could be significant, ex.\n",
    "  'Respondent is curious about smoking and is exposed to it within the household'. Just a suggestion.\n",
    "- In hindsight, i overlooked the importance of people potentially using flavored nicotine products vs non-flavored.\n",
    "- Used Q39 and Q100,101 as target label\n",
    "\n",
    "After that I decided to do some data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unneccessary rows (manually determined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed, in 'drop_list' and ignore any missing columns\n",
    "df_new = df.drop(columns=dropped, errors='ignore')\n",
    "\n",
    "# Calculate the number of columns before and after dropping\n",
    "original_column_count = df.shape[1]\n",
    "new_column_count = df_new.shape[1]\n",
    "columns_dropped = original_column_count - new_column_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns dropped: {columns_dropped}\")\n",
    "print(f\"Original columns: {original_column_count}, New columns: {new_column_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove all rows that have a TEXT value response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Identify and drop columns that contain 'TEXT' in their column IDs\n",
    "text_columns = [col for col in df_new.columns if 'TEXT' in col]\n",
    "df_new_notext = df_new.drop(columns=text_columns, errors='ignore')\n",
    "\n",
    "# Calculate and print the result for 'TEXT' columns\n",
    "original_column_count = df_new.shape[1]\n",
    "new_column_count = df_new_notext.shape[1]\n",
    "columns_dropped = original_column_count - new_column_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns dropped: {columns_dropped}\")\n",
    "print(f\"Original columns: {original_column_count}, New columns: {new_column_count}\")\n",
    "\n",
    "df_new_notext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for and display all columns with missing values. \n",
    "Select `View as a scrollable element` to exit truncated view see all columns. By displaying all the columns with missing values and their counts, we can see which ones are due to a skip because a previous question disqualified them from this one or the question not applying to them: they are the ones with the very high amount of missing values per question / column. The smaller amounts are most often because of edit errors or just not answered / not displayed (what does that mean?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_new_notext.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert all columns to numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any columns contain numeric-like data stored as strings (do not run yet! WIP) \n",
    "for column in df.columns:\n",
    "    # Ensure the column is of object (string-like) type\n",
    "    if df[column].dtype == 'object':\n",
    "        # Now safely apply the str accessor\n",
    "        if df[column].str.isnumeric().any():\n",
    "            print(f\"Column {column} contains numeric-like data but is stored as a string.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert all string data into numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in df.columns:\n",
    "    # Ensure the column is of object (string-like) type\n",
    "    if df[column].dtype == 'object':\n",
    "        # Check if it contains numeric-like data stored as strings\n",
    "        if df[column].str.isnumeric().any():\n",
    "            # Convert numeric-like strings to actual numbers\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        else:\n",
    "            # Convert categorical strings to numerical labels using LabelEncoder\n",
    "            df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "print(\"All string-based categorical columns have been converted to numerical values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify all string based columns have been converted to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    # Ensure the column is of object (string-like) type\n",
    "    if df[column].dtype == 'object':\n",
    "        # Now safely apply the str accessor\n",
    "        if df[column].str.isnumeric().any():\n",
    "            print(f\"Column {column} contains numeric-like data but is stored as a string.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "- Make appropriate transformations in map_annotated before handling missing values.\n",
    "  1. Merge columns together into new columns and drop all the old ones that were previously separate. Similar columns become one umbrella column. Ex. \"Respondent used e-cigarettes due to exposure from friends, media, or family\" encompasses multiple columns and reduces dimensions.\n",
    "  2. Use one-hot encoding to separate a categorical labeled columns into separate columns for each category. Ex. Ages 0-13, 14-18, 19+ become their own categories.\n",
    "  3. Consider combining attributes together that aren't necessarily similar but may be correlated.\n",
    "- Then handle missing values with a high missing rate by replacing it with 0. The reasoning is because:\n",
    "\n",
    "In the case of QN4: \"QN4B: Are you Hispanic, Latino, Latina, or of Spanish origin? (Yes, Mexican, Mexican American, Chicano, or Chicana)\" a missing response indicates that they are not what the question is asking.\n",
    "So in cases like such, you would not use a median or mean value as it's only 1 or No response (Yes or No).\n",
    "You would also not just drop the column because over 50% of participants left it empty. It just means that 50% or more of participants are not what the question is asking. \n",
    " \n",
    "Another example is QN7: \"QN7: How old were you when you first used an e-cigarette, even once or twice?\". This also has a very high number of missing values for this column. That is because the previous question asks if you have ever used it. So all those who indicated no would skip this question. 0 would represent an absence of the behavior of using an e-cigarette.\n",
    "\n",
    "Assuming the first model we use is multilinear regression, we would ideally want all values to be binary (0/1), so these processes such as one hot encoding and merging and replacing values with 0 after strategically dropping irrelevant columns is preparing us for that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging columns into new columns and drop old columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NC1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before merging\n",
    "qn11_columns = ['QN11a', 'QN11b', 'QN11f']  \n",
    "qn12_columns = ['QN12a', 'QN12b', 'QN12f']  \n",
    "print(\"Before NC1 Column Creation:\")\n",
    "print(df[qn11_columns + qn12_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_influence(row):\n",
    "    # if there is a value in those two columns it means they are influenced\n",
    "    if row[qn11_columns + qn12_columns].any(): \n",
    "        #we can return 1 to indicate influence\n",
    "        return \"1\"\n",
    "    # and 0 for no influence\n",
    "    # returning zero here, we can also fill NA values alongside\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "# Apply the function to create a new Influence column\n",
    "df['NC1'] = df.apply(check_influence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after merging:\n",
    "print(\"After NC1 Column Creation:\")\n",
    "print(df[qn11_columns + qn12_columns + ['NC1']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we can drop those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns used for the first NC1 merge\n",
    "nc1_columns = ['QN11a', 'QN11b', 'QN11b', 'QN12a', 'QN12b', 'QN12f']\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df = df.drop(columns=nc1_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before merge\n",
    "nc2_columns = ['QN12l']\n",
    "\n",
    "print(\"Before NC2 Column Creation:\")\n",
    "print(df[nc2_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mental_state(row):\n",
    "    if row[nc2_columns].any():\n",
    "        return \"1\"  # Bad mental state present\n",
    "    else:\n",
    "        return \"0\"  # No bad mental state\n",
    "\n",
    "# Apply the function to create NC2\n",
    "df['NC2'] = df.apply(check_mental_state, axis=1)\n",
    "# After merging, display NC2 column alongside related columns\n",
    "print(\"After NC2 Column Creation:\")\n",
    "print(df[nc2_columns + ['NC2']].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_drop = ['QN12c', 'QN12d', 'QN12e', 'QN12g', 'QN12h', 'QN12i', 'QN12j', 'QN12k', 'QN12l','QN12m', 'QN12n']\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before merging\n",
    "qn16_columns = ['QN16']  \n",
    "qn17_columns = ['QN17']  \n",
    "print(\"Before NC3 Column Creation:\")\n",
    "print(df[qn16_columns + qn17_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nc3_columns = qn16_columns + qn17_columns\n",
    "\n",
    "# Define the function to check for nicotine use\n",
    "def check_nicotine_use(row):\n",
    "    if row[nc3_columns].any():  # Checks if there's any value in QN16 or QN17\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "# Apply the function to create NC3\n",
    "df['NC3'] = df.apply(check_nicotine_use, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After NC3 Column Creation:\")\n",
    "print(df[qn16_columns + qn17_columns + ['NC3']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we drop the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc3_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now merging NC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC4, including QN77\n",
    "nc4_columns = ['QN18e_a', 'QN18e_b', 'QN18e_c', 'QN18e_d', 'QN18e_e', \n",
    "               'QN18e_f', 'QN18e_g', 'QN18e_h', 'QN18e_i', 'QN18e_j', 'QN18e_k', 'QN77']\n",
    "\n",
    "print(\"Before NC4 Column Creation:\")\n",
    "print(df[nc4_columns].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nicotine_pouch_use(row):\n",
    "    # If there's a value in any nc4_columns, it indicates nicotine pouch use\n",
    "    if row[nc4_columns].any():\n",
    "        return \"1\"  # Nicotine pouch use present\n",
    "    else:\n",
    "        return \"0\"  # No nicotine pouch use\n",
    "    \n",
    "\n",
    "# Apply the function to create NC4\n",
    "df['NC4'] = df.apply(check_nicotine_pouch_use, axis=1)\n",
    "\n",
    "\n",
    "print(\"After NC4 Column Creation:\")\n",
    "print(df[nc4_columns + ['NC4']].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the rest of the columns that were used to create NC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc4_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now merging NC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC5, including QN83\n",
    "nc5_columns = ['QN18f_a', 'QN18f_b', 'QN18f_c', 'QN18f_d', 'QN18f_e', \n",
    "               'QN18f_f', 'QN18f_g', 'QN18f_h', 'QN18f_i', 'QN18f_j', 'QN18f_k', 'QN83']\n",
    "\n",
    "print(\"Before NC5 Column Creation:\")\n",
    "print(df[nc5_columns].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_oral_nicotine_use(row):\n",
    "    # If there's a value in any nc5_columns, it indicates oral nicotine use\n",
    "    if row[nc5_columns].any():\n",
    "        return \"1\"  # Oral nicotine use present\n",
    "    else:\n",
    "        return \"0\"  # No oral nicotine use\n",
    "\n",
    "\n",
    "# Apply the function to create NC5\n",
    "df['NC5'] = df.apply(check_oral_nicotine_use, axis=1)\n",
    "\n",
    "print(\"After NC5 Column Creation:\")\n",
    "print(df[nc5_columns + ['NC5']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns that made NC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc5_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now merging NC6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC6\n",
    "nc6_columns = ['QN21a_a', 'QN21a_b', 'QN21a_c', 'QN21a_d', 'QN21a_e', \n",
    "               'QN21a_f', 'QN21a_g']\n",
    "\n",
    "print(\"Before NC6 Column Creation:\")\n",
    "print(df[nc6_columns].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ecig_access(row):\n",
    "    # If there's a value in any nc6_columns, it indicates e-cigarette access\n",
    "    if row[nc6_columns].any():\n",
    "        return \"1\"  # E-cigarette access present\n",
    "    else:\n",
    "        return \"0\"  # No e-cigarette access\n",
    "\n",
    "\n",
    "# Apply the function to create NC6\n",
    "df['NC6'] = df.apply(check_ecig_access, axis=1)\n",
    "\n",
    "print(\"After NC6 Column Creation:\")\n",
    "print(df[nc6_columns + ['NC6']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc6_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now merging NC7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for NC7\n",
    "nc7_columns = ['QN30', 'QN31', 'QN32', 'QN33']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC7 Column Creation:\")\n",
    "print(df[nc7_columns].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to check for curiosity about e-cigarettes\n",
    "def check_curiosity(row):\n",
    "    # If there's a value in any nc7_columns, it indicates curiosity or openness\n",
    "    if row[nc7_columns].any():\n",
    "        return \"1\"  # Curiosity or openness present\n",
    "    else:\n",
    "        return \"0\"  # No curiosity or openness\n",
    "    \n",
    "\n",
    "# Apply the function to create NC7\n",
    "df['NC7'] = df.apply(check_curiosity, axis=1)\n",
    "\n",
    "print(\"After NC7 Column Creation:\")\n",
    "print(df[nc7_columns + ['NC7']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns we used to make NC7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc7_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now merging NC8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC8\n",
    "nc8_columns = ['QN34a', 'QN34b', 'QN34c', 'QN35a', 'QN35b', 'QN35c']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC8 Column Creation:\")\n",
    "print(df[nc8_columns].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check for vaping marijuana, CBD, or THC products\n",
    "def check_vaping_substances(row):\n",
    "    # If there's a value in any nc8_columns, it indicates vaping of these substances\n",
    "    if row[nc8_columns].any():\n",
    "        return \"1\"  # Vaping these substances present\n",
    "    else:\n",
    "        return \"0\"  # No vaping of these substances\n",
    "    \n",
    "\n",
    "\n",
    "# Apply the function to create NC8\n",
    "df['NC8'] = df.apply(check_vaping_substances, axis=1)\n",
    "print(\"After NC8 Column Creation:\")\n",
    "print(df[nc8_columns + ['NC8']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc8_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merging columns  'QN135A', 'QN135B', 'QN135C', 'QN135D', and 'QN135E'  to create NC11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC11\n",
    "nc9_columns = ['QN135a', 'QN135b', 'QN135c', 'QN135d', 'QN135e']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC11 Column Creation:\")\n",
    "print(df[nc9_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check if e-cigarette use was witnessed at school\n",
    "def check_ecig_witnessed(row):\n",
    "    # If there's a value in any nc11_columns, it indicates e-cigarette use witnessed at school\n",
    "    if row[nc9_columns].any():\n",
    "        return \"1\"  # E-cigarette use witnessed\n",
    "    else:\n",
    "        return \"0\"  # No e-cigarette use witnessed\n",
    "\n",
    "\n",
    "# Apply the function to create NC11\n",
    "df['NC9'] = df.apply(check_ecig_witnessed, axis=1)\n",
    "print(\"After NC9 Column Creation:\")\n",
    "print(df[nc9_columns + ['NC9']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc9_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merging columns 'QN136B', 'QN136C', 'QN136D', 'QN136E', 'QN136H', \n",
    "                'QN136I', 'QN136J', 'QN136K', and 'QN136L' to create NC12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC12\n",
    "nc10_columns = ['QN136b', 'QN136c', 'QN136d', 'QN136e', 'QN136h', \n",
    "                'QN136i', 'QN136j', 'QN136k', 'QN136l']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC10 Column Creation:\")\n",
    "print(df[nc10_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check if respondent lives with someone who uses tobacco products\n",
    "def check_tobacco_use_at_home(row):\n",
    "    # If there's a value in any nc12_columns, it indicates tobacco use at home\n",
    "    if row[nc10_columns].any():\n",
    "        return \"1\"  # Tobacco use present in household\n",
    "    else:\n",
    "        return \"0\"  # No tobacco use in household\n",
    "\n",
    "\n",
    "# Apply the function to create NC12\n",
    "df['NC10'] = df.apply(check_tobacco_use_at_home, axis=1)\n",
    "print(\"After NC10 Column Creation:\")\n",
    "print(df['NC10'].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc10_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merging columns 'QN137A', 'QN137B', 'QN137C', 'QN137D', and 'QN137E' to create NC13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC11\n",
    "nc11_columns = ['QN137a', 'QN137b', 'QN137c', 'QN137d', 'QN137e']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC11 Column Creation:\")\n",
    "print(df[nc11_columns].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check for experiences of psychological distress or discrimination at school\n",
    "def check_school_distress(row):\n",
    "    # If there's a value in any nc13_columns, it indicates distress or discrimination experience\n",
    "    if row[nc11_columns].any():\n",
    "        return \"1\"  # Psychological distress or discrimination present\n",
    "    else:\n",
    "        return \"0\"  # No psychological distress or discrimination\n",
    "\n",
    "# Apply the function to create NC13\n",
    "df['NC11'] = df.apply(check_school_distress, axis=1)\n",
    "print(\"After NC11 Column Creation:\")\n",
    "print(df['NC11'].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns that created NC13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc11_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merging columns 'QN137H', 'QN137I', 'QN137J', 'QN137K', 'QN137L', 'QN137M', and 'QN137N' to create NC14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for NC12\n",
    "nc12_columns = ['QN137h', 'QN137i', 'QN137j', 'QN137k', 'QN137l', 'QN137m', 'QN137n']\n",
    "\n",
    "# Print the columns before merging\n",
    "print(\"Before NC12 Column Creation:\")\n",
    "print(df[nc12_columns].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check for general experiences of discrimination\n",
    "def check_discrimination(row):\n",
    "    # If there's a value in any nc14_columns, it indicates discrimination experience\n",
    "    if row[nc12_columns].any():\n",
    "        return \"1\"  # Discrimination experience present\n",
    "    else:\n",
    "        return \"0\"  # No discrimination experience\n",
    "\n",
    "# Apply the function to create NC14\n",
    "df['NC12'] = df.apply(check_discrimination, axis=1)\n",
    "print(\"After NC12 Column Creation:\")\n",
    "print(df[nc12_columns + ['NC12']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the rest of the columns used to create NC14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=nc12_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying all new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_columns = ['NC1', 'NC2', 'NC3', 'NC4', 'NC5', 'NC6', 'NC7', 'NC8', 'NC9', 'NC10', 'NC11', 'NC12']\n",
    "print(df[nc_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only QN or NC columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with columns that start with 'QN' or contain 'NC'\n",
    "df_qn_nc = df.filter(regex='(^QN|NC)')\n",
    "print(list(df_qn_nc.columns))\n",
    "# save into df\n",
    "df = df_qn_nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns by parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed, in 'drop_list' and ignore any missing columns\n",
    "df_new = df.drop(columns=dropped, errors='ignore')\n",
    "\n",
    "# Calculate the number of columns before and after dropping\n",
    "original_column_count = df.shape[1]\n",
    "new_column_count = df_new.shape[1]\n",
    "columns_dropped = original_column_count - new_column_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns dropped: {columns_dropped}\")\n",
    "print(f\"Original columns: {original_column_count}, New columns: {new_column_count}\")\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now handle the rest of the values that are NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Column  NA_Count\n",
      "0       artificial_id         0\n",
      "1     Non_SOGI_School         0\n",
      "2            Location        69\n",
      "3                 QN1        90\n",
      "4                 QN2       152\n",
      "...               ...       ...\n",
      "1464              PSU         0\n",
      "1465          PSU_num         0\n",
      "1466      WT_analysis         0\n",
      "1467           QN141R      4200\n",
      "1468           QN142R      3991\n",
      "\n",
      "[1469 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display NA counts in a DataFrame format\n",
    "na_counts_df = df.isna().sum().reset_index()\n",
    "na_counts_df.columns = ['Column', 'NA_Count']\n",
    "print(na_counts_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle missing values. \n",
    "In our dataset, there are some naturally missing values due to branching of the questions. Those missing values are significantly higher than actual missing values(respondent did not answer). \n",
    "Our approach is to replace the naturally missing values with -1, and use that as an indicator when we train the model; for questions respondent did not answer, we fill those with 0, indicating an actual missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2993\n",
    "fill_value = -1\n",
    "high_na_columns = df.columns[df.isna().sum() > threshold]\n",
    "df[high_na_columns] = df[high_na_columns].fillna(fill_value)\n",
    "\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QN6'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "filter through the rest of the na columns and fill with mean/median/mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill_mode = ['QN1', 'QN2', 'QN3', 'QN117', 'QN119', 'QN120', \n",
    "                        'QN127', 'QN128', 'QN131', 'QN132', 'QN140', \n",
    "                        'QN148', 'QN149']\n",
    "\n",
    "# Fill each column with its mode\n",
    "for col in columns_to_fill_mode:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the rest of the columns that have na still here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Location', 'QN1', 'QN2', 'QN3', 'QN4a', 'QN4b', 'QN4c', 'QN4d', 'QN4e',\n",
       "       'QN5a',\n",
       "       ...\n",
       "       'CHOOKAH', 'CROLLCIGTS', 'CPIPE', 'CSNUS', 'CORAL', 'CBIDIS', 'CHTP',\n",
       "       'CPOUCH', 'QN141R', 'QN142R'],\n",
       "      dtype='object', length=1248)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin of model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forrest Model**\n",
    "\n",
    "Since we have a dataset that is based on a questionnaire, the most straightforward approach is to use a decision tree. \n",
    "However, since our dataset contains responses from over 1k respondents, and over 150 questions, our dataset is no long fit for a decision tree due to the size of our dataset. \n",
    "\n",
    "Hence, our apporach is to use a random forrest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial_id         0\n",
      "Non_SOGI_School       0\n",
      "Location             69\n",
      "QN1                  90\n",
      "QN2                 152\n",
      "                   ... \n",
      "PSU                   0\n",
      "PSU_num               0\n",
      "WT_analysis           0\n",
      "QN141R             4200\n",
      "QN142R             3991\n",
      "Length: 1469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create indicator columns for -1 values (skipped responses)\n",
    "for col in df.columns:\n",
    "    if (df[col] == -1).sum() > 0:  # Check for columns containing -1 values\n",
    "        df[col + '_skipped'] = (df[col] == -1).astype(int)  # Create indicator column\n",
    "\n",
    "X = df.drop(columns=['QN100'])\n",
    "y = df['QN100']\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Initialize and train the RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optional: Display feature importance\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
